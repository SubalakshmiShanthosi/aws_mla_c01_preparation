# S3 Service

S3 - Event Notification Setup 

Example usecase - Employee images are uploaded on images/ folder on s3 bucket: employee-directory-image-bucket

Steps required to generate thumbnail images uploaded and stored on output/ folder

Step 1 - Incorporate SQS to recieve all object change events from S3

    Steps - 
    * Creation of bucket - Default Configuration
    * Enable EventBridge notification
    * Create Event Notification --> Send message to SQS
    * Side tab - Create an SQS queue with default configuration 
    * Back on S3 Event notification creation tab - Look for this created SQS queue
    * While event creation - error with finding sqs service
    * Come back to SQS tab - Edit SQS Resource Policy 
            Principal - * Allow
            Action - SendMessage
            ARN - SQS Queue ARN 
    * Now S3 event notification is created on S3 bucket


__S3 Baseline Performance__ - When multipart
    Big files >100MB files must use for > 5GB

__S3 transfer acceleration__ - File is transferred to an edge location which will forward data to S3 bucker in target region
Minimise amount of private internet

__S3 Byte range fetches__ - GET parallize requests by byte range 

__Amazon S3 Encryption__ - 4 types

SSE-S3, SSE-KMS, SSE-C, Client Side encryption 

Exam perspective - what usecase need what kind of encryption

x-amz-server-side-encryption:AES256 --> For SSE-S3
x-amz-server-side-encryption:"aws:kms"

Shortcomings while using KMS - Impact by KMS limits --> 
KMS quota /region bight be 5500,10000, 30000 but for high throughput or access intensive S3 bucket might lead to throttling

SSE-KMS can use default KMS key created for S3 service while using this type of encryption from console. 
We can also create an custom KMS key and use it on the S3 bucket upon creation/ updating encryption configuration.
SSE-KMS Bucket Key? what is happening to reduce amount of calls to KMS service? 




SSE-C --> Client passes the encryption key along in header while sending object every HTTP request made

SSE-C option selection can only be done from __CLI__ not from console as for every HTTP request the encryption key need to be sent

Amazon S3 does NOT store encryption key provided while object upload

HTTPS protocol must be used for SSE-C type encryption

Client Side Encryption --> Clients must encrypt data themselves before sending to Amazon S3

Clients must take care of encryption before uploading the file to S3

While accessing files on S3 -> in Client side encryption customer will have to decrypt outside of S3

Encryption in transit --> SSL/TLS 
HTTPS endpoint is having encryption on flight

Force Encryption --> aws:SecureTransport: false

Bucket policy to restrict HTTP access calls


Default encryption vs Bucket policies -

SSE-S3 is default encryption policy
Force encryption bucket policy is the Specification for S3 bucket a policy that will refuce any API call to PUT an S3 object without encryption headers(SSE-KMS or SSE-C)

__S3 Access Points__

Based on Bucket => Access point Policy Grant R/W to /folder1 prefix

Access points simplifies the security management of S3 buckets 
Each Access points has -> it's own DNS name, (Internet origin or VPC)
Access point policy -- manage security at scale


Example scenario Assume having compute logic on EC2, EC2 instance need to access S3 bucket which has access point is of __only VPC Origin__

In that case - 
Step1 - Create EC2 subnets private and public subnet
Create Route table for public subnet

Create security groups for ALLOW or DENY setup for resources
Create an route table endpoint to public subnet --> internet gateway (public)
Create an endpoint                        instance (private) --> 
Create an VPC endpoint for S3
 Specify the VPC route and endpoint (private created previous step appropriate route table entry)
    Get into policy creation tool and allow S3 getobject and listbucket 
Recreate EC2 instance with new settings







